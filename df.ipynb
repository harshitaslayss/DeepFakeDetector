{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5949419286440719\n",
      "Validation Accuracy: 73.86%\n",
      "Epoch 2, Loss: 0.4153774505989118\n",
      "Validation Accuracy: 68.18%\n",
      "Epoch 3, Loss: 0.3506501540541649\n",
      "Validation Accuracy: 75.00%\n",
      "Epoch 4, Loss: 0.2751523981040174\n",
      "Validation Accuracy: 67.05%\n",
      "Epoch 5, Loss: 0.30833838372067973\n",
      "Validation Accuracy: 76.14%\n",
      "Epoch 6, Loss: 0.23656564934009855\n",
      "Validation Accuracy: 75.00%\n",
      "Epoch 7, Loss: 0.21047663815658202\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch 8, Loss: 0.182336325884204\n",
      "Validation Accuracy: 77.27%\n",
      "Epoch 9, Loss: 0.17252596611665053\n",
      "Validation Accuracy: 75.00%\n",
      "Epoch 10, Loss: 0.17277906284752217\n",
      "Validation Accuracy: 76.14%\n"
     ]
    }
   ],
   "source": [
    "# Define a custom dataset\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None, num_frames=16):\n",
    "        self.real_dir = real_dir\n",
    "        self.fake_dir = fake_dir\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # Get list of video paths and labels\n",
    "        self.real_videos = [os.path.join(real_dir, fname) for fname in os.listdir(real_dir)]\n",
    "        self.fake_videos = [os.path.join(fake_dir, fname) for fname in os.listdir(fake_dir)]\n",
    "        self.video_paths = self.real_videos + self.fake_videos\n",
    "        self.labels = [0] * len(self.real_videos) + [1] * len(self.fake_videos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        frames = self.extract_frames(video_path)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label\n",
    "\n",
    "    def extract_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, max(1, total_frames) - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            else:\n",
    "                frame = np.zeros((224, 224, 3), dtype=np.uint8)  # Padding with black frames\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "# Use ResNet-18 as feature extractor\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.size()\n",
    "        x = x.view(-1, C, H, W)  # Merge batch and frames\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(batch_size, num_frames)\n",
    "        x = x.mean(dim=1).unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "# Define transformations with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "real_dir = r'C:\\Users\\Harshita\\Desktop\\deepfake\\FF++\\real'\n",
    "fake_dir = r'C:\\Users\\Harshita\\Desktop\\deepfake\\FF++\\fake'\n",
    "dataset = VideoDataset(real_dir, fake_dir, transform=transform)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset.labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetModel().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'fake_video_detection_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset file at: .gradio\\flagged\\dataset2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1635, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Harshita\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Harshita\\AppData\\Local\\Temp\\ipykernel_1552\\3105082388.py\", line 39, in predict_video\n",
      "    frames = extract_frames(video_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Harshita\\AppData\\Local\\Temp\\ipykernel_1552\\3105082388.py\", line 4, in extract_frames\n",
      "    cap = cv2.VideoCapture(video_path)\n",
      "          ^^^\n",
      "NameError: name 'cv2' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "# Ensure this function exists BEFORE predict_video()\n",
    "def extract_frames(video_path, num_frames=16):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, max(1, total_frames) - 1, num_frames, dtype=int)\n",
    "\n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            frame = np.zeros((224, 224, 3), dtype=np.uint8)  # Black frame fallback\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def predict_video(video):\n",
    "    video_path = video  # Ensure path handling\n",
    "    frames = extract_frames(video_path)  \n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames).unsqueeze(0).to(device)  # Ensure correct shape\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(frames)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "    \n",
    "    confidence = round(probability * 100, 2)\n",
    "    if probability > 0.4:\n",
    "        return f\"Fake Video ({confidence}% confidence)\"\n",
    "    else:\n",
    "        return f\"Real Video ({100 - confidence}% confidence)\"\n",
    "\n",
    "def predict_video(video):\n",
    "    video_path = video if isinstance(video, str) else video.name  # Ensure path handling\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames).unsqueeze(0).to(device)  # Ensure correct shape\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(frames)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "    confidence = round(probability * 100, 2)\n",
    "    if probability > 0.5:\n",
    "        return f\"Fake Video ({confidence}% confidence)\"\n",
    "    else:\n",
    "        return f\"Real Video ({100 - confidence}% confidence)\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict_video,\n",
    "    inputs=gr.Video(label=\"Upload Video\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Deepfake Video Detector\",\n",
    "    description=\"Upload a video to check if it's real or fake.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
